<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>CogNav</title>
  <link rel="icon" type="image/x-icon" href="static/images/icon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

<style>
  .video-container {
    width: 30%; /* 每个容器宽度为页面宽度的32%，根据实际情况调整 */
    display: inline-block; /* 使容器并列排列 */
    margin: 1%; /* 添加外边距 */
    vertical-align: top;
  }

  video {
    width: 90%; /* 视频宽度填满容器 */
    display: inline-block; /* 确保视频占满整个容器宽度 */
    margin: 0 auto; /* 视频居中 */
  }

  .description {
    text-align: center; /* 文字居中对齐 */
  }
  .scale-image {
      width: 80%; /* 设定特定图片的宽度 */
      height: auto; /* 保持宽高比 */
      transition: transform 0.3s ease; /* 添加动画过渡效果 */
  }
  .text-image-container {
    display: flex; /* 使用弹性盒模型布局 */
    align-items: flex-start; /* 上端对齐 */
    justify-content: center;
}

ul {
  text-align: justify; /* 确保文本左对齐 */
  list-style-position: inside; /* 列表标记与文本对齐 */
  font-size: 16px;
  list-style-type: circle; /* 使用圆形作为项目符号 */

}


</style>




<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">CogNav: Cognitive Process Modeling for Object Goal Navigation with LLMs</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <!-- <span class="author-block"><a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Jiazhao Zhang</a><sup>1,2,*</sup>,</span> -->
              <span class="author-block">
                Yihan Cao<sup>1,*</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://jzhzhang.github.io/">Jiazhao Zhang</a><sup>2,*</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                Zhinan Yu<sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                Shuzhen Liu<sup>1</sup>
              </span>
              <span class="author-block" style="display: block;">
                <a href="https://scholar.google.cl/citations?user=DnHBAN0AAAAJ&hl=zh-CN">Zheng Qin</a><sup>3</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://scholar.google.com.hk/citations?hl=en&user=dJ8izFAAAAAJ&view_op=list_works&sortby=pubdate">Qin Zou</a><sup>4</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://scholar.google.com/citations?user=Shy1gnMAAAAJ&hl=zh-CN">Bo Du</a><sup>4</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://kevinkaixu.net/">Kai Xu</a><sup>1,†</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <!-- <span class="author-block">Institution Name<br>Conferance name and year</span> -->
              <span class="author-block"><sup>1</sup>
                National University of Defense Technology&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <sup>2</sup>CFCS, School of Computer Science, Peking University&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              </span>
              <span class="author-block" style="display: block;">
                <sup>3</sup>Defense Innovation Institute, Academy of Military Sciences &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <sup>4</sup>School of Computer Science, Wuhan University &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              </span>
              <span class="eql-cntrb" style="display: block;">
                <small><sup>*</sup>Indicates Equal Contribution,&nbsp;</small>
                <small><sup>†</sup>Indicates Equal Advising.</small>
              </span>
            </div>




            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2402.15852.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
                </span>

                <!-- Supplementary PDF link -->
                <!-- <span class="link-block">
                  <a href="https://www.youtube.com/watch?v=IHkDuJZV0I8" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-video"></i>
                  </span>
                  <span>Video</span>
                </a>
                </span> -->

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/YOUR REPO HERE" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code (soon)</span>
                </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/2402.15852" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                  </a>
                </span> -->
              </div>
            </div>
  
            <!-- <div class="text">
              Robotics: Science and Systems (RSS 2024)
            </div> -->

          </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <div class="text-image-container title is-3">
              <div>
                <img src="static/images/logo.png" alt="示例图片" width="45">
              </div>
              <div class="text">
                  <p>Contribution</p>
              </div>
            </div>
            </h2>
            <ul>
              <li><b>CogNav is the first attempt of cognitive process modeling for ObjectNav via exploiting the commonsense and spatial reasoning capability of LLMs.</b></li>
              <li><b>CogNav designs the cognitive states and the prompts for grounded reasoning of state transitions with an LLM.</b></li>
              <li><b>CogNav builds a heterogeneous cognitive map representation which is constructed online and can be corrected through prompting an LLM to ensure high map accuracy.</b></li>
              <li><b>CogNav achieves state-of-the-art (SOTA) performance in both simulated and real-world environments, and exhibits strong generalizability on unseen scenarios.</b></li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero is-small">

  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
      <div class="column is-four-fifths"></div>
        <h2>
        <h2 class=""> <img src="static/images/highlight_logo.png" width="50"> Highlights</h2>
        <div class="text-image-container title is-3">
          <div>
            <img src="static/images/highlight_logo.png" alt="示例图片" width="50">
          </div>
          <div class="text">
              <p>Highlights</p>
          </div>
        </div>
        </h2>
        <ul>
          <li><b>CogNav is the first attempt of cognitive process modeling for ObjectNav via exploiting the commonsense and spatial reasoning capability of LLMs.</b></li>
          <li><b>CogNav designs the cognitive states and the prompts for grounded reasoning of state transitions with an LLM.</b></li>
          <li><b>CogNav builds a heterogeneous cognitive map representation which is constructed online and can be corrected through prompting an LLM to ensure high map accuracy.</b></li>
          <li><b>CogNav achieves state-of-the-art (SOTA) performance in both simulated and real-world environments, and exhibits strong generalizability on unseen scenarios.</b></li>
        </ul>
      </div>

    </div>

  </div>
  
</section> -->


<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
      <h2 class="title is-3"> Demos of CogNav in Real-world Environment</h2>
      <!-- <h2 class="title is-5"> (R2R training split ->  RxR validation unseen split )</h2> -->
      <div id="results-carousel-teaser1" class="carousel results-carousel">

        <div class="item item-video16">
          <video poster="" id="video16" autoplay playsinline controls muted  height="100%">
            <source src="static/videos/real_video_caption/真机-bed.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video17">
          <video poster="" id="video17" autoplay playsinline controls muted  height="100%">
            <source src="static/videos/real_video_caption/真机-cup.mp4"
            type="video/mp4">
          </video>
        </div>
      
      </div>
    </div>
  </div>
</section>




<!-- Teaser video 1 -->
<!-- <section class="hero is-small">

  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">




      <h2 class="title is-3">Sim-to-Real Demos: Simple Instruction VLN</h2>
      <p> <b>In these demos, the agent navigates following relatively simple instructions, such as walking to a single landmark. NaVid demonstrates the ability to accurately distinguish differences in similar instructions and accordingly complete precise navigation behaviors.</b></p>
      <p> <b>Real-world demos by following simple instructions, such as walking to a single landmark.</b></p>

      <div id="results-carousel-teaser1" class="carousel results-carousel">





        <div class="item item-video7">
          <video poster="" id="video7" autoplay playsinline controls muted loop height="100%">
            <source src="static/videos/teaser/simple_instruction_1_compressed.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video8">
          <video poster="" id="video8" autoplay playsinline controls muted loop height="100%">
            <source src="static/videos/teaser/simple_instruction_2_compressed.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video9">
          <video poster="" id="video9" autoplay playsinline controls muted loop height="100%">
            <source src="static/videos/teaser/simple_instruction_3_compressed.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>

    </div>
  </div>
</section> -->
<!-- End teaser video 1 -->

<!-- Teaser video 2 -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
      <h2 class="title is-3">Sim-to-Real Demos: Complex Instruction VLN</h2>
      <p> <b>In these demos, the agent navigates according to complex instructions composed of multiple simple instructions in sequence. NaVid can accurately execute them in the correct order.</b></p>

      <div id="results-carousel-teaser2" class="carousel results-carousel">
        <div class="item item-video10">
          <video poster="" id="video10" autoplay playsinline controls muted loop height="100%">
            <source src="static/videos/teaser/complex_instruction_1_compressed.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video11">
          <video poster="" id="video11" autoplay playsinline controls muted loop height="100%">
            <source src="static/videos/teaser/complex_instruction_2_compressed.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video12">
          <video poster="" id="video12" autoplay playsinline controls muted loop height="100%">
            <source src="static/videos/teaser/complex_instruction_3_compressed.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    
    </div>
  </div>
</section> -->
<!-- End teaser video 2 -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Object goal navigation (ObjectNav) is a fundamental task of embodied AI that requires the agent to find a target object in unseen environments. This task is particularly challenging as it demands both perceptual and cognitive processes for effective perception and decision-making. While perception has gained significant progress powered by the rapidly developed visual foundation models, the progress on the cognitive side remains limited to either implicitly learning from massive navigation demonstrations or explicitly leveraging pre-defined heuristic rules.Inspired by neuroscientific evidence that humans consistently update their cognitive states while searching for objects in unseen environments, we present CogNav, which attempts to model this cognitive process with the help of large language models. Specifically, we model the cognitive process with a finite state machine composed of cognitive states ranging from exploration to identification. The transitions between the states are determined by a large language model based on an online built heterogeneous cognitive map containing spatial and semantic information of the scene being explored.Extensive experiments on both synthetic and real-world environments demonstrate that our cognitive modeling significantly improves ObjectNav efficiency, with human-like navigation behaviors. In an open-vocabulary and zero-shot setting, our method advances the SOTA of the HM3D benchmark from 69.3% to 87.2%. The code and data will be released. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- YouTube Video-->



<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <h2 class="title is-3">Summary Video</h2>
            <div class="publication-video">
              -TODO Dhruv: put video link here
              <iframe src="https://www.youtube.com/embed/IHkDuJZV0I8?rel=0&amp;showinfo=0" frameborder="0"
                allow="autoplay; encrypted-media" allowfullscreen></iframe>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->






<!-- Method Overview -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <h2 class="title is-3">Method Overview</h2>
            <img src="static/images/pipeline.png" alt="CogNav" class="center-image blend-img-background">
            <div class="level-set has-text-justified">
              <p class="has-text-justified">
                <b>The overview of CogNav.</b> Our method takes a posed RGB-D sequence as input and incrementally constructs an online cognitive map, comprising a scene graph, a landmark graph, and an occupancy map. We then perform cognitive map prompting by encoding cognitive information and goal object into a text prompt used to query the LLM to determine the next cognitive state. Based on the state, the LLM is queried again to select a landmark to guide the robot. A deterministic local planner is used to generate a path to the selected landmark.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <h2 class="title is-3">Examples of Cognitive State Transition</h2>
            <img src="static/images/visualization.png" alt="CogNav" class="center-image blend-img-background">
            <div class="level-set has-text-justified">
              <!-- <p class="has-text-justified">
                <b>We encode knowledge from the cognitive map by constructing landmark-centered prompts encompassing both scene and agent information.</b>
              </p>
              <p class="has-text-justified"> -->
                <!-- <b>We initialize the encoders and Vicuna-7B using pre-trained weights, and our model requires only one epoch for the training process.</b> -->
              <!-- </p> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>





<!-- End Method Overview  -->

<!-- Image carousel Caption  -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
          <h2 class="title is-3">Caption Results Visualization</h2>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item"><img src="static/images/caption/caption-1.png" alt="MY ALT TEXT"/></div>
            <div class="item"><img src="static/images/caption/caption-2.png" alt="MY ALT TEXT"/></div>
            <div class="item"><img src="static/images/caption/caption-3.png" alt="MY ALT TEXT"/></div>
            <div class="item"><img src="static/images/caption/caption-4.png" alt="MY ALT TEXT"/></div>
            <div class="item"><img src="static/images/caption/caption-5.png" alt="MY ALT TEXT"/></div>
            <div class="item"><img src="static/images/caption/caption-6.png" alt="MY ALT TEXT"/></div>
          </div>
    </div>
  </div>
</section> -->
<!-- End image carousel Caption  -->

<!-- Video carousel 1 Caption -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
      <h2 class="title is-3">Results of CogNav in simulated environment</h2>
      <h2 class="title is-3">Caption Results Visualization</h2>
        <p> <b>Object-goal navigation on HM3D and MP3D datasets in Habitat Simulator. </b></p>

      <div class="video-container">
        <video autoplay playsinline controls muted loop>
          <source src="./static/videos/sim_video_caption/1.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <div class="description"><b>NaVid:</b> Walk forward and turn left. Wait by the first door on the left.</div>
      </div>
      
      <div class="video-container">
        <video autoplay playsinline controls muted loop>
          <source src="./static/videos/sim_video_caption/2.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <div class="description"><b>NaVid:</b> Walk forward and turn left. Wait near the first doorway.</div>
      </div>
      
      <div class="video-container">
        <video autoplay playsinline controls muted loop>
          <source src="./static/videos/sim_video_caption/3.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <div class="description"><b>NaVid:</b> Turn around and walk to the other side of the room. Wait by the chair and table.</div>
      </div>

      <div class="video-container">
        <video autoplay playsinline controls muted loop>
          <source src="./static/videos/sim_video_caption/1-mp3d.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <div class="description"><b>NaVid:</b> Exit the bathroom. Turn left and enter the bedroom. Wait near the bed.</div>
      </div>
      
      <div class="video-container">
        <video autoplay playsinline controls muted loop>
          <source src="./static/videos/sim_video_caption/2-mp3d.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <div class="description"><b>NaVid:</b> Walk through the doorway and turn left. Walk past the pool and wait by the first chair.</div>
      </div>
      
      <div class="video-container">
        <video autoplay playsinline controls muted loop>
          <source src="./static/videos/sim_video_caption/3-mp3d.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <div class="description"><b>NaVid:</b> Turn around and go into the hallway. Go into the room with the stairs and turn right. Go into the room with the kitchen
          and wait there.</div>


      </div>
  </div>
</section> -->
<!-- End Video carousel 1 Caption -->

<!-- Image carousel 2 R2R -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
      <h2 class="title is-3">An Example of Cognitive State Transition</h2>
      <div id="results-carousel" class="carousel results-carousel">  
        <div class="item"><img class="scale-image" src="static/images/nav/1.png" alt="MY ALT TEXT"/></div>
        <div class="item"><img class="scale-image" src="static/images/nav/2.png" alt="MY ALT TEXT"/></div>
        <div class="item"><img class="scale-image" src="static/images/nav/3.png" alt="MY ALT TEXT"/></div>
        <div class="item"><img class="scale-image" src="static/images/nav/4.png" alt="MY ALT TEXT"/></div>
        <div class="item"><img class="scale-image" src="static/images/nav/5.png" alt="MY ALT TEXT"/></div>
        <div class="item"><img class="scale-image" src="static/images/nav/6.png" alt="MY ALT TEXT"/></div>
      </div>
    </div>
  </div>
</section> -->
<!-- End image carousel 2 R2R -->


<!-- End video carousel R2R -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
      <h2 class="title is-3">Results of CogNav on HM3D in Habitat Simulator</h2>
      <!-- <h3 class="title is-5">Results on HM3D</h3> -->
      <div id="results-carousel-teaser1" class="carousel results-carousel">

        <div class="item item-video10">
          <video poster="" id="video10" autoplay playsinline controls muted  height="100%">
            <source src="static/videos/sim_video_caption/1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video11">
          <video poster="" id="video11" autoplay playsinline controls muted  height="100%">
            <source src="static/videos/sim_video_caption/2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video12">
          <video poster="" id="video12" autoplay playsinline controls muted  height="100%">
            <source src="static/videos/sim_video_caption/3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
      <h2 class="title is-3">Results of CogNav on MP3D in Habitat Simulator</h2>
      <!-- <h3 class="title is-5">Results on HM3D</h3> -->
      <div id="results-carousel-teaser1" class="carousel results-carousel">

        <div class="item item-video10">
          <video poster="" id="video10" autoplay playsinline controls muted  height="100%">
            <source src="static/videos/sim_video_caption/1-mp3d.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video11">
          <video poster="" id="video11" autoplay playsinline controls muted  height="100%">
            <source src="static/videos/sim_video_caption/2-mp3d.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video12">
          <video poster="" id="video12" autoplay playsinline controls muted  height="100%">
            <source src="static/videos/sim_video_caption/3-mp3d.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Image carousel 3 RxR -->

<!-- End image carousel 3 RxR -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <!-- <pre><code>@article{zhang2024navid,
        title={NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation},
        author={Zhang, Jiazhao and Wang, Kunyu and Xu, Rongtao and Zhou, Gengze and Hong, Yicong and Fang, Xiaomeng and Wu, Qi and Zhang, Zhizheng and Wang, He},
        journal={arXiv preprint arXiv:2402.15852},
        year={2024}
      }</code></pre> -->
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
